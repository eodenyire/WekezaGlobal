# ============================================================
#  WekezaGlobal Infrastructure (WGI) — Logstash Pipeline
#  Tech Stack §4 — ELK Stack: Centralized logging & auditing
#  Security Model §5 — "All system, transaction, and API activity logged for 7+ years"
# ============================================================

input {
  # Beats input — receives logs from Filebeat or Metricbeat agents
  beats {
    port => 5044
  }

  # TCP syslog input — accepts structured JSON logs over TCP
  tcp {
    port => 5000
    codec => json_lines
  }
}

filter {
  # Parse timestamp from backend structured logs
  if [message] {
    json {
      source  => "message"
      skip_on_invalid_json => true
    }
  }

  # Normalise timestamp
  if [timestamp] {
    date {
      match => ["timestamp", "ISO8601"]
      target => "@timestamp"
      remove_field => ["timestamp"]
    }
  }

  # Tag WGI service name for index routing
  mutate {
    add_field => { "[@metadata][target_index]" => "wgi-logs-%{+YYYY.MM.dd}" }
  }

  # Mask sensitive fields before indexing
  # 1. Redact from the raw message string (for logs not yet JSON-parsed)
  mutate {
    gsub => [
      "message", '"password"\s*:\s*"[^"]*"', '"password":"[REDACTED]"',
      "message", '"otp"\s*:\s*"[^"]*"',      '"otp":"[REDACTED]"',
      "message", '"card_number"\s*:\s*"[^"]*"', '"card_number":"[REDACTED]"'
    ]
  }
  # 2. Remove top-level fields that were parsed from JSON (Security Model §3 — Sensitive Data Masking)
  mutate {
    remove_field => ["password", "password_hash", "otp", "card_number", "account_number", "personal_id"]
  }
}

output {
  elasticsearch {
    hosts    => ["elasticsearch:9200"]
    index    => "%{[@metadata][target_index]}"
    action   => "index"
  }

  # Uncomment for local debug:
  # stdout { codec => rubydebug }
}
